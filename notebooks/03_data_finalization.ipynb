{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa03ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad188f51",
   "metadata": {},
   "source": [
    "Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153c0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = \"../data/processed\"\n",
    "output_data_dir = \"../data/outputs\"\n",
    "\n",
    "os.makedirs(output_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462d9af",
   "metadata": {},
   "source": [
    "Load/Align Rasters Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b342b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_align_rasters(file_list, reference_raster=None):\n",
    "    \"\"\"\n",
    "    Loads a list of rasters and aligns them to a common resolution/extent.\n",
    "    If reference_raster is provided, aligns all to that raster.\n",
    "    Returns an xarray.Dataset with all rasters stacked as variables.\n",
    "    \"\"\"\n",
    "    rasters = []\n",
    "    names = []\n",
    "\n",
    "    for file in file_list:\n",
    "        arr = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "        names.append(os.path.splitext(os.path.basename(file))[0])\n",
    "\n",
    "        # Reproject/align if reference is given\n",
    "        if reference_raster is not None:\n",
    "            arr = arr.rio.reproject_match(reference_raster)\n",
    "\n",
    "        rasters.append(arr)\n",
    "\n",
    "    # Stack into dataset\n",
    "    ds = xr.merge([rasters[i].to_dataset(name=names[i]) for i in range(len(rasters))])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c96769",
   "metadata": {},
   "source": [
    "Severity Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b83893d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_severity(dnbr):\n",
    "    if dnbr < 0.1:\n",
    "        return \"Unburned\"\n",
    "    elif dnbr < 0.27:\n",
    "        return \"Low\"\n",
    "    elif dnbr < 0.44:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"High\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4668478",
   "metadata": {},
   "source": [
    "Process All Fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c76454d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Bootleg fire ...\n",
      "Skipping Bootleg — no dNBR available.\n",
      "Processing Caldor fire ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John Waugh\\AppData\\Local\\Temp\\ipykernel_17020\\1609434101.py:21: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n",
      "  ds = xr.merge([rasters[i].to_dataset(name=names[i]) for i in range(len(rasters))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Caldor dataset with 5517951 pixels → ../data/outputs\\Caldor_dataset.parquet\n",
      "Processing Camp fire ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John Waugh\\AppData\\Local\\Temp\\ipykernel_17020\\1609434101.py:21: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n",
      "  ds = xr.merge([rasters[i].to_dataset(name=names[i]) for i in range(len(rasters))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Camp dataset with 4412816 pixels → ../data/outputs\\Camp_dataset.parquet\n",
      "Processing Dixie fire ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John Waugh\\AppData\\Local\\Temp\\ipykernel_17020\\1609434101.py:21: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n",
      "  ds = xr.merge([rasters[i].to_dataset(name=names[i]) for i in range(len(rasters))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Dixie dataset with 5625635 pixels → ../data/outputs\\Dixie_dataset.parquet\n",
      "Processing Troublesome fire ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John Waugh\\AppData\\Local\\Temp\\ipykernel_17020\\1609434101.py:21: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n",
      "  ds = xr.merge([rasters[i].to_dataset(name=names[i]) for i in range(len(rasters))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Troublesome dataset with 5576320 pixels → ../data/outputs\\Troublesome_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "all_fire_dfs = []\n",
    "\n",
    "# Detect fires based on available files\n",
    "fires = sorted(set(os.path.basename(f).split(\"_\")[0] for f in glob(os.path.join(processed_data_dir, \"*.tif\"))))\n",
    "\n",
    "for fire_name in fires:\n",
    "    print(f\"Processing {fire_name} fire ...\")\n",
    "\n",
    "    fire_files = glob(os.path.join(processed_data_dir, f\"{fire_name}_*.tif\"))\n",
    "    dnbr_path = os.path.join(processed_data_dir, f\"{fire_name}_dNBR.tif\")\n",
    "\n",
    "    if not os.path.exists(dnbr_path):\n",
    "        print(f\"Skipping {fire_name} — no dNBR available.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        reference = rxr.open_rasterio(dnbr_path, masked=True).squeeze()\n",
    "        fire_ds = load_and_align_rasters(fire_files, reference_raster=reference)\n",
    "\n",
    "        # Mask no-data areas based on dNBR\n",
    "        fire_ds = fire_ds.where(~np.isnan(fire_ds[f\"{fire_name}_dNBR\"]), drop=True)\n",
    "\n",
    "        # Convert to dataframe\n",
    "        df = fire_ds.to_dataframe().reset_index()\n",
    "        df = df.dropna()\n",
    "        df[\"fire_name\"] = fire_name\n",
    "        df[\"severity\"] = df[f\"{fire_name}_dNBR\"].apply(classify_severity)\n",
    "\n",
    "        # Save individual file\n",
    "        fire_output = os.path.join(output_data_dir, f\"{fire_name}_dataset.parquet\")\n",
    "        df.to_parquet(fire_output, index=False)\n",
    "        print(f\"Saved {fire_name} dataset with {len(df)} pixels → {fire_output}\")\n",
    "\n",
    "        all_fire_dfs.append(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fire_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a482a49e",
   "metadata": {},
   "source": [
    "Combine Fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afaa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset created: 42,265,444 rows across 5 events\n",
      "Columns: ['latitude', 'longitude', 'dNBR', 'SPI', 'VCI', 'NDVI', 'severity', 'fire_name']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "severity\n",
       "Unburned    39231620\n",
       "Low          2135624\n",
       "Moderate      496776\n",
       "High          401424\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_datasets = list(Path(output_data_dir).glob('*_dataset.parquet'))\n",
    "\n",
    "combined_list = []\n",
    "\n",
    "for file in fire_datasets:\n",
    "    fire_name = file.stem.replace('_dataset', '')\n",
    "    \n",
    "    df = pd.read_parquet(file)\n",
    "    \n",
    "    df.columns = [re.sub(f\"^{fire_name}_\", \"\", c) for c in df.columns]\n",
    "    \n",
    "    df = df.rename(columns={\n",
    "        'x': 'longitude',\n",
    "        'y': 'latitude',\n",
    "        'veg_indices': 'NDVI'\n",
    "    })\n",
    "    \n",
    "    if 'fire_name' not in df.columns:\n",
    "        df['fire_name'] = fire_name\n",
    "    else:\n",
    "        df['fire_name'] = df['fire_name'].fillna(fire_name)\n",
    "    \n",
    "    expected_cols = ['latitude', 'longitude', 'dNBR', 'SPI', 'VCI', 'NDVI', 'severity', 'fire_name']\n",
    "    available_cols = [c for c in expected_cols if c in df.columns]\n",
    "    df = df[available_cols]\n",
    "    \n",
    "    combined_list.append(df)\n",
    "\n",
    "combined_df = pd.concat(combined_list, ignore_index=True)\n",
    "\n",
    "combined_df.to_parquet('combined_dataset.parquet', index=False)\n",
    "\n",
    "print(f\"Combined dataset created: {len(combined_df):,} rows across {len(fire_datasets)} events\")\n",
    "print(\"Columns:\", list(combined_df.columns))\n",
    "\n",
    "combined_df.head()\n",
    "combined_df['fire_name'].value_counts()\n",
    "combined_df['severity'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
