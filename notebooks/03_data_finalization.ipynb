{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa03ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad188f51",
   "metadata": {},
   "source": [
    "Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153c0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = \"../data/processed\"\n",
    "output_data_dir = \"../data/outputs\"\n",
    "\n",
    "os.makedirs(output_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462d9af",
   "metadata": {},
   "source": [
    "Load/Align Rasters Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b342b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_align_rasters(file_list, reference_raster=None):\n",
    "    \"\"\"\n",
    "    Loads a list of rasters and aligns them to a common resolution/extent.\n",
    "    If reference_raster is provided, aligns all to that raster.\n",
    "    Returns an xarray.Dataset with all rasters stacked as variables.\n",
    "    \"\"\"\n",
    "    rasters = []\n",
    "    names = []\n",
    "\n",
    "    for file in file_list:\n",
    "        arr = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "        names.append(os.path.splitext(os.path.basename(file))[0])\n",
    "\n",
    "        # Reproject/align if reference is given\n",
    "        if reference_raster is not None:\n",
    "            arr = arr.rio.reproject_match(reference_raster)\n",
    "\n",
    "        rasters.append(arr)\n",
    "\n",
    "    # Stack into dataset\n",
    "    ds = xr.merge([rasters[i].to_dataset(name=names[i]) for i in range(len(rasters))])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c96769",
   "metadata": {},
   "source": [
    "Severity Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b83893d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_severity(dnbr):\n",
    "    if dnbr < 0.1:\n",
    "        return \"Unburned\"\n",
    "    elif dnbr < 0.27:\n",
    "        return \"Low\"\n",
    "    elif dnbr < 0.44:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"High\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4668478",
   "metadata": {},
   "source": [
    "Process All Fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c76454d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Bootleg fire ...\n",
      "Skipping Bootleg — no dNBR available.\n",
      "Processing Caldor fire ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John Waugh\\AppData\\Local\\Temp\\ipykernel_12704\\1609434101.py:21: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n",
      "  ds = xr.merge([rasters[i].to_dataset(name=names[i]) for i in range(len(rasters))])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m fire_ds = fire_ds.where(~np.isnan(fire_ds[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfire_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_dNBR\u001b[39m\u001b[33m\"\u001b[39m]), drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Convert to dataframe\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m df = \u001b[43mfire_ds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.reset_index()\n\u001b[32m     25\u001b[39m df = df.dropna()\n\u001b[32m     26\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mfire_name\u001b[39m\u001b[33m\"\u001b[39m] = fire_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John Waugh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xarray\\core\\dataset.py:7256\u001b[39m, in \u001b[36mDataset.to_dataframe\u001b[39m\u001b[34m(self, dim_order)\u001b[39m\n\u001b[32m   7228\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert this dataset into a pandas.DataFrame.\u001b[39;00m\n\u001b[32m   7229\u001b[39m \n\u001b[32m   7230\u001b[39m \u001b[33;03mNon-index variables in this dataset form the columns of the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   7251\u001b[39m \n\u001b[32m   7252\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   7254\u001b[39m ordered_dims = \u001b[38;5;28mself\u001b[39m._normalize_dim_order(dim_order=dim_order)\n\u001b[32m-> \u001b[39m\u001b[32m7256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John Waugh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xarray\\core\\dataset.py:7199\u001b[39m, in \u001b[36mDataset._to_dataframe\u001b[39m\u001b[34m(self, ordered_dims)\u001b[39m\n\u001b[32m   7190\u001b[39m extension_array_columns_same_index = [\n\u001b[32m   7191\u001b[39m     k\n\u001b[32m   7192\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m extension_array_columns\n\u001b[32m   7193\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m extension_array_columns_different_index\n\u001b[32m   7194\u001b[39m ]\n\u001b[32m   7195\u001b[39m data = [\n\u001b[32m   7196\u001b[39m     \u001b[38;5;28mself\u001b[39m._variables[k].set_dims(ordered_dims).values.reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m   7197\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m non_extension_array_columns\n\u001b[32m   7198\u001b[39m ]\n\u001b[32m-> \u001b[39m\u001b[32m7199\u001b[39m index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m*\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7200\u001b[39m broadcasted_df = pd.DataFrame(\n\u001b[32m   7201\u001b[39m     {\n\u001b[32m   7202\u001b[39m         **\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(non_extension_array_columns, data, strict=\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m   7208\u001b[39m     index=index,\n\u001b[32m   7209\u001b[39m )\n\u001b[32m   7210\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m extension_array_column \u001b[38;5;129;01min\u001b[39;00m extension_array_columns_different_index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John Waugh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xarray\\core\\coordinates.py:187\u001b[39m, in \u001b[36mAbstractCoordinates.to_index\u001b[39m\u001b[34m(self, ordered_dims)\u001b[39m\n\u001b[32m    183\u001b[39m         level_list += [\u001b[38;5;28mlist\u001b[39m(level) \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m levels]\n\u001b[32m    184\u001b[39m         names += index.names\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.MultiIndex(\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     levels=level_list, codes=[\u001b[38;5;28mlist\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m code_list], names=names\n\u001b[32m    188\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_fire_dfs = []\n",
    "\n",
    "# Detect fires based on available files\n",
    "fires = sorted(set(os.path.basename(f).split(\"_\")[0] for f in glob(os.path.join(processed_data_dir, \"*.tif\"))))\n",
    "\n",
    "for fire_name in fires:\n",
    "    print(f\"Processing {fire_name} fire ...\")\n",
    "\n",
    "    fire_files = glob(os.path.join(processed_data_dir, f\"{fire_name}_*.tif\"))\n",
    "    dnbr_path = os.path.join(processed_data_dir, f\"{fire_name}_dNBR.tif\")\n",
    "\n",
    "    if not os.path.exists(dnbr_path):\n",
    "        print(f\"Skipping {fire_name} — no dNBR available.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        reference = rxr.open_rasterio(dnbr_path, masked=True).squeeze()\n",
    "        fire_ds = load_and_align_rasters(fire_files, reference_raster=reference)\n",
    "\n",
    "        # Mask no-data areas based on dNBR\n",
    "        fire_ds = fire_ds.where(~np.isnan(fire_ds[f\"{fire_name}_dNBR\"]), drop=True)\n",
    "\n",
    "        # Convert to dataframe\n",
    "        df = fire_ds.to_dataframe().reset_index()\n",
    "        df = df.dropna()\n",
    "        df[\"fire_name\"] = fire_name\n",
    "        df[\"severity\"] = df[f\"{fire_name}_dNBR\"].apply(classify_severity)\n",
    "\n",
    "        # Save individual file\n",
    "        fire_output = os.path.join(output_data_dir, f\"{fire_name}_dataset.parquet\")\n",
    "        df.to_parquet(fire_output, index=False)\n",
    "        print(f\"Saved {fire_name} dataset with {len(df)} pixels → {fire_output}\")\n",
    "\n",
    "        all_fire_dfs.append(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fire_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a482a49e",
   "metadata": {},
   "source": [
    "Combine Fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0afaa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 fire datasets to combine.\n",
      "\n",
      "[1/9] Processing Caldor_dataset.parquet\n",
      "   → Done (5,517,951 rows total so far)\n",
      "[2/9] Processing Camp_dataset.parquet\n",
      "   → Done (11,127,381 rows total so far)\n",
      "[3/9] Processing Carr_dataset.parquet\n",
      "   → Done (16,807,517 rows total so far)\n",
      "[4/9] Processing Creek_dataset.parquet\n",
      "   → Done (22,234,915 rows total so far)\n",
      "[5/9] Processing Dixie_dataset.parquet\n",
      "   → Done (27,860,550 rows total so far)\n",
      "[6/9] Processing Glass_dataset.parquet\n",
      "   → Done (31,693,542 rows total so far)\n",
      "[7/9] Processing Thomas_dataset.parquet\n",
      "   → Done (36,151,792 rows total so far)\n",
      "[8/9] Processing Troublesome_dataset.parquet\n",
      "   → Done (41,728,112 rows total so far)\n",
      "[9/9] Processing Woolsey_dataset.parquet\n",
      "   → Done (43,819,782 rows total so far)\n",
      "\n",
      "Combined dataset created successfully: combined_dataset.parquet\n",
      "Total rows combined: 43,819,782\n"
     ]
    }
   ],
   "source": [
    "fire_datasets = list(Path(output_data_dir).glob('*_dataset.parquet'))\n",
    "output_file = Path(\"combined_dataset.parquet\")\n",
    "\n",
    "if output_file.exists():\n",
    "    output_file.unlink()\n",
    "\n",
    "writer = None\n",
    "total_rows = 0\n",
    "\n",
    "print(f\"Found {len(fire_datasets)} fire datasets to combine.\\n\")\n",
    "\n",
    "for i, file in enumerate(fire_datasets, start=1):\n",
    "    fire_name = file.stem.replace('_dataset', '')\n",
    "    print(f\"[{i}/{len(fire_datasets)}] Processing {file.name}\")\n",
    "\n",
    "    dataset = ds.dataset(file, format=\"parquet\")\n",
    "\n",
    "    for batch in dataset.to_batches():\n",
    "        df = batch.to_pandas()\n",
    "\n",
    "        df.columns = [re.sub(f\"^{fire_name}_\", \"\", c) for c in df.columns]\n",
    "\n",
    "        rename_map = {\n",
    "            'x': 'longitude',\n",
    "            'y': 'latitude',\n",
    "            'veg_indices': 'NDVI'\n",
    "        }\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "        severity_cols = [c for c in df.columns if 'severity' in c.lower()]\n",
    "        if severity_cols:\n",
    "            df = df.rename(columns={severity_cols[0]: 'severity'})\n",
    "        else:\n",
    "            df['severity'] = pd.NA\n",
    "\n",
    "        df['fire_name'] = fire_name\n",
    "\n",
    "        expected_cols = ['latitude', 'longitude', 'dNBR', 'SPI', 'VCI', 'NDVI', 'severity', 'fire_name']\n",
    "        for col in expected_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = pd.NA\n",
    "\n",
    "        df = df[expected_cols]\n",
    "\n",
    "        for col in ['dNBR', 'SPI', 'VCI', 'NDVI']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce', downcast='float')\n",
    "        \n",
    "        if 'severity' in df.columns:\n",
    "            df['severity'] = df['severity'].astype('string')\n",
    "\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(output_file, table.schema)\n",
    "        writer.write_table(table)\n",
    "\n",
    "        total_rows += len(df)\n",
    "\n",
    "    print(f\"   → Done ({total_rows:,} rows total so far)\")\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"\\nCombined dataset created successfully: {output_file}\")\n",
    "print(f\"Total rows combined: {total_rows:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
