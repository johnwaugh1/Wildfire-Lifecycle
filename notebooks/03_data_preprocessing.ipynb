{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f980211",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7821a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.warp import reproject, calculate_default_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8687a",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c39e15eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found fire folders: ['../data/raw\\\\bootleg_fire', '../data/raw\\\\caldor_fire', '../data/raw\\\\camp_fire', '../data/raw\\\\carr_fire', '../data/raw\\\\dixie_fire', '../data/raw\\\\east_troublesome_fire', '../data/raw\\\\glass_fire', '../data/raw\\\\red_salmon_fire', '../data/raw\\\\zogg_fire']\n"
     ]
    }
   ],
   "source": [
    "raw_dir = \"../data/raw\"\n",
    "processed_dir = \"../data/processed\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "fire_folders = [f for f in glob.glob(os.path.join(raw_dir, \"*\")) if os.path.isdir(f)]\n",
    "print(\"Found fire folders:\", fire_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90d12e",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4ac4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_align(path, ref_profile):\n",
    "    \"\"\"Load raster, reproject/align to reference profile, \n",
    "    and enforce correct dtype for categorical rasters.\"\"\"\n",
    "    \n",
    "    categorical_layers = [\"landcover\", \"modis_burned_area\"]\n",
    "\n",
    "    name = os.path.basename(path).lower()\n",
    "\n",
    "    is_categorical = any(cat in name for cat in categorical_layers)\n",
    "\n",
    "    # Choose resampling based on type\n",
    "    resample = rasterio.enums.Resampling.nearest if is_categorical else rasterio.enums.Resampling.bilinear\n",
    "\n",
    "    with rasterio.open(path) as src:\n",
    "        data = src.read(1)\n",
    "\n",
    "        # Reproject\n",
    "        dst = np.empty((ref_profile[\"height\"], ref_profile[\"width\"]), dtype=\"float32\")\n",
    "\n",
    "        rasterio.warp.reproject(\n",
    "            source=data,\n",
    "            destination=dst,\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=ref_profile[\"transform\"],\n",
    "            dst_crs=ref_profile[\"crs\"],\n",
    "            dst_resolution=(ref_profile[\"transform\"][0], -ref_profile[\"transform\"][4]),\n",
    "            resampling=resample,\n",
    "        )\n",
    "\n",
    "    # Enforce integer categories\n",
    "    if is_categorical:\n",
    "        dst = np.rint(dst).astype(np.int16)\n",
    "\n",
    "    return dst\n",
    "    \n",
    "def get_reference_profile(fire_dir):\n",
    "    dem_path = os.path.join(fire_dir, \"srtm_dem.tif\")\n",
    "    if not os.path.exists(dem_path):\n",
    "        raise ValueError(f\"No DEM found in {fire_dir}\")\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        return src.profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc5da5",
   "metadata": {},
   "source": [
    "Burn Severity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abb73783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_dnbr(dnbr):\n",
    "    \"\"\"\n",
    "    MTBS burn severity classification based on dNBR.\n",
    "    \"\"\"\n",
    "    classes = np.full(dnbr.shape, -1, dtype=np.int16)\n",
    "\n",
    "    classes[(dnbr < 0.1)] = 0\n",
    "    classes[(dnbr >= 0.1) & (dnbr < 0.27)] = 1\n",
    "    classes[(dnbr >= 0.27) & (dnbr < 0.66)] = 2\n",
    "    classes[(dnbr >= 0.66)] = 3\n",
    "    \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39de2c",
   "metadata": {},
   "source": [
    "Post Processing Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d11dbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(df, feature_schema):\n",
    "    report = {}\n",
    "    \n",
    "    for col, expected in feature_schema.items():\n",
    "        if col not in df.columns:\n",
    "            report[col] = \"Missing column\"\n",
    "            continue\n",
    "\n",
    "        series = df[col]\n",
    "\n",
    "        # Check for NaN or Inf\n",
    "        nan_count = series.isna().sum()\n",
    "        inf_count = np.isinf(series).sum()\n",
    "\n",
    "        # Continuous variable ranges\n",
    "        if expected != (\"categorical\",):\n",
    "            low, high = expected\n",
    "            out_of_range = ((series < low) | (series > high)).sum()\n",
    "            report[col] = {\n",
    "                \"type\": \"continuous\",\n",
    "                \"nan\": int(nan_count),\n",
    "                \"inf\": int(inf_count),\n",
    "                \"out_of_range\": int(out_of_range),\n",
    "                \"min\": float(series.min()),\n",
    "                \"max\": float(series.max())\n",
    "            }\n",
    "        else:\n",
    "            # For categorical, just check non-integer or negative values\n",
    "            invalid = (~series.dropna().astype(float).apply(float.is_integer)).sum()\n",
    "            negative = (series < 0).sum()\n",
    "            report[col] = {\n",
    "                \"type\": \"categorical\",\n",
    "                \"nan\": int(nan_count),\n",
    "                \"inf\": int(inf_count),\n",
    "                \"non_integer\": int(invalid),\n",
    "                \"negative\": int(negative),\n",
    "                \"unique_values\": sorted(series.unique().tolist()[:20])\n",
    "            }\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672243d8",
   "metadata": {},
   "source": [
    "Main Preprocessing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a28b36c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Processing bootleg_fire #\n",
      "  Validation passed for bootleg_fire.\n",
      "  Added 6908032 samples\n",
      "\n",
      "# Processing caldor_fire #\n",
      "  Validation passed for caldor_fire.\n",
      "  Added 4139624 samples\n",
      "\n",
      "# Processing camp_fire #\n",
      "  Validation passed for camp_fire.\n",
      "  Added 3103047 samples\n",
      "\n",
      "# Processing carr_fire #\n",
      "  Validation passed for carr_fire.\n",
      "  Added 3313036 samples\n",
      "\n",
      "# Processing dixie_fire #\n",
      "  Validation passed for dixie_fire.\n",
      "  Added 8281472 samples\n",
      "\n",
      "# Processing east_troublesome_fire #\n",
      "  Validation passed for east_troublesome_fire.\n",
      "  Added 3311549 samples\n",
      "\n",
      "# Processing glass_fire #\n",
      "  Validation passed for glass_fire.\n",
      "  Added 1242110 samples\n",
      "\n",
      "# Processing red_salmon_fire #\n",
      "  Validation passed for red_salmon_fire.\n",
      "  Added 2206710 samples\n",
      "\n",
      "# Processing zogg_fire #\n",
      "  Validation passed for zogg_fire.\n",
      "  Added 3448449 samples\n",
      "\n",
      "Final Dataset Size: 35954029\n",
      "\n",
      "Saved dataset to ../data/processed\\stacked_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "# Validation schema\n",
    "feature_schema = {\n",
    "    \"ndvi_pre\": (-1, 1),\n",
    "    \"ndvi_post\": (-1, 1),\n",
    "    \"dndvi\": (-2, 2),\n",
    "    \"nbr_pre\": (-1, 1),\n",
    "    \"nbr_post\": (-1, 1),\n",
    "    \"dnbr\": (-2, 2),\n",
    "    \"precip\": (0, 300),       \n",
    "    \"temp\": (230, 330),     \n",
    "    \"landcover\": (\"categorical\",),\n",
    "    \"elevation\": (-200, 9000),\n",
    "    \"severity\": (\"categorical\",)\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for fire_dir in fire_folders:\n",
    "    fire_name = os.path.basename(fire_dir)\n",
    "    print(f\"\\n# Processing {fire_name} #\")\n",
    "\n",
    "    try:\n",
    "        ref_profile = get_reference_profile(fire_dir)\n",
    "    except:\n",
    "        print(f\"  MISSING DEM - skipping {fire_name}\")\n",
    "        continue\n",
    "\n",
    "    def load(name):\n",
    "        p = os.path.join(fire_dir, name)\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"  Missing {name} - skipping fire.\")\n",
    "            return None\n",
    "        return load_and_align(p, ref_profile)\n",
    "\n",
    "    # Load features\n",
    "    ndvi_pre = load(\"pre_ndvi.tif\")\n",
    "    ndvi_post = load(\"post_ndvi.tif\")\n",
    "    nbr_pre = load(\"pre_nbr.tif\")\n",
    "    nbr_post = load(\"post_nbr.tif\")\n",
    "    chirps = load(\"chirps_precip.tif\")\n",
    "    era5 = load(\"era5_temp.tif\")\n",
    "    landcover = load(\"landcover.tif\")\n",
    "    dem = load(\"srtm_dem.tif\")\n",
    "    modis = load(\"modis_burned_area.tif\")\n",
    "\n",
    "    # Skip if any are missing\n",
    "    if any(x is None for x in [ndvi_pre, ndvi_post, nbr_pre, nbr_post, chirps, era5, landcover, dem, modis]):\n",
    "        print(\"  Missing required layers - skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Derived features\n",
    "    dndvi = ndvi_pre - ndvi_post\n",
    "    dnbr = nbr_pre - nbr_post\n",
    "    severity = classify_dnbr(dnbr)\n",
    "\n",
    "    # Flatten valid pixels\n",
    "    df = pd.DataFrame({\n",
    "        \"fire_name\": fire_name,\n",
    "        \"ndvi_pre\": ndvi_pre.flatten(),\n",
    "        \"ndvi_post\": ndvi_post.flatten(),\n",
    "        \"dndvi\": dndvi.flatten(),\n",
    "        \"nbr_pre\": nbr_pre.flatten(),\n",
    "        \"nbr_post\": nbr_post.flatten(),\n",
    "        \"dnbr\": dnbr.flatten(),\n",
    "        \"precip\": chirps.flatten(),\n",
    "        \"temp\": era5.flatten(),\n",
    "        \"landcover\": landcover.flatten(),\n",
    "        \"elevation\": dem.flatten(),\n",
    "        \"severity\": severity.flatten(),\n",
    "    })\n",
    "\n",
    "    # Drop invalid\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    df = df[df[\"severity\"] >= 0]\n",
    "\n",
    "    # Validate fire's dataset\n",
    "    fire_report = validate_dataset(df, feature_schema)\n",
    "\n",
    "    # Detect critical failures\n",
    "    critical_fail = False\n",
    "    for feat, stats in fire_report.items():\n",
    "        if stats == \"Missing column\":\n",
    "            print(f\"  Validation Failed: missing {feat}\")\n",
    "            critical_fail = True\n",
    "            break\n",
    "\n",
    "        # Continuous: check for out of range values\n",
    "        if stats[\"type\"] == \"continuous\":\n",
    "            if stats[\"out_of_range\"] > 0:\n",
    "                print(f\"  Feature `{feat}` has {stats['out_of_range']} out-of-range values\")\n",
    "                critical_fail = True\n",
    "\n",
    "        # Categorical: check for negative or non-integer values\n",
    "        if stats[\"type\"] == \"categorical\":\n",
    "            if stats[\"non_integer\"] > 0 or stats[\"negative\"] > 0:\n",
    "                print(f\"  Feature `{feat}` has invalid categorical values ({stats})\")\n",
    "                critical_fail = True\n",
    "\n",
    "    if critical_fail:\n",
    "        print(f\"  Skipping {fire_name} due to failed validation.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  Validation passed for {fire_name}.\")\n",
    "\n",
    "    print(f\"  Added {len(df)} samples\")\n",
    "\n",
    "    all_data.append(df)\n",
    "\n",
    "# Combine across all fires\n",
    "full_df = pd.concat(all_data, ignore_index=True)\n",
    "print(\"\\nFinal Dataset Size:\", len(full_df))\n",
    "\n",
    "full_path = os.path.join(processed_dir, \"stacked_dataset.parquet\")\n",
    "full_df.to_parquet(full_path)\n",
    "\n",
    "print(\"\\nSaved dataset to\", full_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
